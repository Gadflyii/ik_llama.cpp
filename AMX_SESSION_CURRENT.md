# AMX Implementation - Current Session Status
**Date**: 2025-10-03
**Time**: Late evening continued - Option B implementation in progress
**Status**: üü° PARTIAL PROGRESS - Buffer list infrastructure complete, tensor allocation integration needed

---

## üìö Documentation Cleanup Summary

**Completed**: Removed 8 outdated documentation files, kept 5 current files

**Files REMOVED** (outdated from earlier sessions):
- AMX_ARCHITECTURE_ANALYSIS.md (old port analysis)
- AMX_CLEAN_PORT_COMPLETE.md (old migration doc)
- AMX_IMPLEMENTATION_LOG.md (partial progress log)
- AMX_IMPLEMENTATION_PLAN.md (old tensor_traits plan)
- AMX_ISSUES_TRACKER.md (duplicate of AMX_ISSUES.md)
- AMX_PORT_STATUS.md (failed port attempt status)
- AMX_SESSION_SUMMARY.md (superseded by this file)
- AMX_STATUS.md (massive 22K outdated file)

**Files KEPT** (current, created 2025-10-03):
1. **AMX_SESSION_CURRENT.md** (this file) - Current session status
2. **AMX_BUFFER_SYSTEM_PORT.md** - Detailed architecture comparison
3. **AMX_NEXT_SESSION_INSTRUCTIONS.md** - Next session handoff
4. **AMX_ISSUES.md** - Issue tracker with 11 issues
5. **AMX_IMPLEMENTATIONS_COMPREHENSIVE_COMPARISON.md** - Comparison of 4 AMX implementations

---

## Session Summary

### üéØ CRITICAL BREAKTHROUGH: Root Cause Identified

**The NaN Issue Was NOT in AMX Kernels - It Was Buffer Allocation!**

**Root Cause**: F32 tensors (embeddings, layer norms) were being allocated in AMX buffers
- `buft_input` (embeddings) was set to AMX buffer type
- `buft_output` (output layer) was set to AMX buffer type
- AMX buffer type doesn't properly support F32 operations
- Result: Layer norm produced NaN ‚Üí First matmul got NaN inputs

**Evidence from Debug Logging**:
```
BEFORE FIX (buft_input = AMX):
Input first 8 values: nan nan nan nan nan nan nan nan  ‚Üê BROKEN

AFTER FIX (buft_input = CPU):
Input first 8 values: 0.000011 0.000130 0.000172 -0.000004  ‚Üê VALID
```

**Still Getting NaN**: But now it's in `ffn_moe_weights_sum-1` (MoE routing), not first matmul inputs
- First matmul works correctly with valid inputs
- Issue moved downstream, suggesting buffer allocation problem affects more than just input/output

---

## Architecture Analysis: Why Upstream Works

### Upstream Buffer System (llama.cpp numa_read_mirror)

**Uses 3-Buffer Architecture**:
1. **CPU_REPACK** (14904 MB) - Repacked weights
2. **AMX** (729 MB) - VNNI-packed Q4_0/Q4_1 weights
3. **CPU_Mapped** (16217 MB) - mmap'd F32 weights
4. **Total**: ~32 GB (full model size)

**Buffer Type Selection**:
```cpp
// Priority order when --no-host is set:
1. ACCEL devices (if any)
2. GPU host buffer ‚Üê SKIPPED when --no-host=true
3. CPU extra buffer types (AMX first, then REPACK)
4. CPU (fallback for F32)

// Each tensor selects best buffer type via:
for (auto buft : buft_list) {
    if (ggml_backend_dev_supports_op(dev, op_tensor)) {
        return buft;  // First match wins
    }
}
```

**Key**: Buffer type list with fallback ensures:
- Q4_0/Q4_1 tensors ‚Üí AMX buffer (supports MUL_MAT)
- F32 tensors ‚Üí CPU buffer (AMX doesn't support, falls back)

### Our Fork (ik_llama.cpp BEFORE this session)

**Used Single-Buffer Architecture**:
1. **CPU** (410 MB) - Some tensors
2. **AMX** (656 MB) - All layer tensors when `--amx` set
3. **Total**: ~1 GB (rest is mmap'd)

**Buffer Type Selection**:
```cpp
// Single buffer type per layer:
model.buft_layer[i] = llama_default_buffer_type_cpu(need_host_buffer, use_amx_buffer);

// When use_amx_buffer=true, returns AMX for ALL tensors in layer
// No fallback mechanism - F32 tensors forced into AMX buffers
```

**Problem**:
- ALL tensors in layer try to use same buffer type
- When `buft_layer = AMX`, F32 tensors have no fallback
- F32 operations in AMX buffer ‚Üí NaN

---

## Solution Implemented: Buffer List System

### Phase 1: ggml-backend.cpp Infrastructure ‚úÖ COMPLETED

**File**: `/home/ron/src/ik_llama.cpp/ggml/src/ggml-backend.cpp`

**Added Extra Buffer Types Registration** (lines 733-752):
```cpp
GGML_CALL ggml_backend_buffer_type_t * ggml_backend_cpu_get_extra_bufts(void) {
    static ggml_backend_buffer_type_t bufts[8] = {NULL};
    static bool initialized = false;

    if (!initialized) {
        int idx = 0;

        #if defined(__AMX_INT8__) && defined(__AVX512VNNI__)
        ggml_backend_buffer_type_t amx_buft = ggml_backend_amx_buffer_type();
        if (amx_buft) {
            bufts[idx++] = amx_buft;  // AMX is FIRST extra buffer type
        }
        #endif

        bufts[idx] = NULL;  // NULL terminator
        initialized = true;
    }

    return bufts;
}
```

**Added Operation Support Check** (lines 755-772):
```cpp
GGML_CALL bool ggml_backend_buft_supports_op(ggml_backend_buffer_type_t buft,
                                               const struct ggml_tensor * op) {
    if (!op || !buft) return false;

    // AMX and other extra buffer types use extra_buffer_type interface
    if (buft->context) {
        return true;  // Assume supported, AMX filters via tensor traits
    }

    return ggml_backend_buft_is_host(buft);  // Host buffers support all ops
}
```

**Header**: `/home/ron/src/ik_llama.cpp/ggml/include/ggml-backend.h`
```cpp
// Line 128: Extra buffer types getter
GGML_API GGML_CALL ggml_backend_buffer_type_t * ggml_backend_cpu_get_extra_bufts(void);

// Line 131: Operation support checker
GGML_API GGML_CALL bool ggml_backend_buft_supports_op(ggml_backend_buffer_type_t buft,
                                                        const struct ggml_tensor * op);
```

### Phase 2: llama.cpp Buffer List System ‚úÖ PARTIALLY COMPLETED

**File**: `/home/ron/src/ik_llama.cpp/src/llama.cpp`

**Added Buffer List Infrastructure** (lines 1785-1836):
```cpp
// Buffer type list typedef
using buft_list_t = std::vector<ggml_backend_buffer_type_t>;

// Select first buffer type that supports operation
static ggml_backend_buffer_type_t select_weight_buft(
    const struct ggml_tensor * tensor,
    ggml_op op,
    const buft_list_t & buft_list)
{
    if (buft_list.empty()) return nullptr;

    for (auto buft : buft_list) {
        if (ggml_backend_buft_supports_op(buft, tensor)) {
            return buft;
        }
    }

    return nullptr;
}

// Create CPU buffer type list
// Priority: AMX (if enabled) ‚Üí Host (if needed) ‚Üí CPU
static buft_list_t make_cpu_buft_list(bool use_extra_bufts, bool no_host) {
    buft_list_t buft_list;

    // 1. Extra buffer types (AMX) if requested
    if (use_extra_bufts) {
        ggml_backend_buffer_type_t * extra_bufts = ggml_backend_cpu_get_extra_bufts();
        while (extra_bufts && *extra_bufts) {
            buft_list.push_back(*extra_bufts);
            ++extra_bufts;
        }
    }

    // 2. Host buffer (for GPU transfers) if needed
    if (!no_host) {
        #ifdef GGML_USE_CUDA
            ggml_backend_buffer_type_t host_buft = ggml_backend_cuda_host_buffer_type();
            if (host_buft) buft_list.push_back(host_buft);
        #elif defined(GGML_USE_SYCL)
            ggml_backend_buffer_type_t host_buft = ggml_backend_sycl_host_buffer_type();
            if (host_buft) buft_list.push_back(host_buft);
        #endif
    }

    // 3. CPU buffer (always fallback)
    buft_list.push_back(ggml_backend_cpu_buffer_type());

    return buft_list;
}
```

**Fixed F32 Buffer Allocation**:
```cpp
// Line 4934: buft_input (embeddings) - NEVER use AMX
model.buft_input = llama_default_buffer_type_cpu(need_host_buffer, false /* no AMX */);

// Line 4978, 5002: buft_output (output layer) - NEVER use AMX
model.buft_output = llama_default_buffer_type_cpu(need_host_buffer, false /* no AMX */);
```

### Phase 3: Tensor Loading Update ‚è≥ TODO (BLOCKING)

**Current Problem**: Our fork still uses old single-buffer model:
```cpp
// Current code (WRONG):
model.buft_layer[i] = llama_default_buffer_type_cpu(need_host_buffer, use_amx_buffer);

// All tensors in layer use this single buffer type
// No per-tensor selection based on operation type
```

**What Needs to Happen**:
```cpp
// Option 1: Change model.buft_layer to buffer list
struct llama_model {
    std::vector<buft_list_t> buft_layer;  // List of buffers for each layer
};

// During tensor allocation:
for (each tensor in layer i) {
    ggml_backend_buffer_type_t buft = select_weight_buft(
        tensor,
        tensor->op,  // MUL_MAT, ADD, etc.
        model.buft_layer[i]
    );
    allocate_tensor_in_buffer(tensor, buft);
}

// Option 2: Select buffer type at layer setup time (SIMPLER)
// Keep model.buft_layer as single type, but select from list intelligently
// Based on predominant tensor type in layer (Q4_0 ‚Üí AMX, F32 ‚Üí CPU)
```

**Files to Modify**:
1. `src/llama.cpp`:
   - Tensor loading in `llm_load_tensors()` (~line 5000-8000)
   - Buffer allocation logic
   - Per-tensor buffer type selection

2. `src/llama-impl.h` (maybe):
   - Model structure if changing `buft_layer` type
   - Only if using Option 1 above

**Estimated Scope**: 200-500 lines of changes

---

## Files Modified This Session

### 1. `/home/ron/src/ik_llama.cpp/ggml/src/ggml-backend.cpp`
**Lines 733-752**: Added `ggml_backend_cpu_get_extra_bufts()`
```cpp
// Returns NULL-terminated array of extra buffer types
// AMX is first in the list (highest priority)
```

**Lines 755-772**: Added `ggml_backend_buft_supports_op()`
```cpp
// Checks if buffer type supports operation
// Used by select_weight_buft to choose buffer type
```

### 2. `/home/ron/src/ik_llama.cpp/ggml/include/ggml-backend.h`
**Line 128**: Declared `ggml_backend_cpu_get_extra_bufts()`
**Line 131**: Declared `ggml_backend_buft_supports_op()`

### 3. `/home/ron/src/ik_llama.cpp/src/llama.cpp`
**Lines 1785-1836**: Added buffer list infrastructure
- `buft_list_t` typedef
- `select_weight_buft()` function
- `make_cpu_buft_list()` function

**Lines 4934, 4978, 5002**: Fixed `buft_input` and `buft_output`
```cpp
// Changed from:
model.buft_input = llama_default_buffer_type_cpu(need_host_buffer, use_amx_buffer);

// To:
model.buft_input = llama_default_buffer_type_cpu(need_host_buffer, false /* no AMX */);
```

### 4. Previous Session Files (Already Modified)
1. `ggml/src/ggml.c` - Barrier implementation
2. `ggml/src/ggml-cpu/amx/amx.cpp` - Backend association, tensor filtering
3. `ggml/src/ggml-cpu/amx/amx.h` - Always available declaration
4. `ggml/src/ggml-cpu/amx/mmq.cpp` - Headers, barrier call
5. `ggml/src/ggml-cpu/amx/common.h` - Correct struct definition
6. `ggml/src/ggml-cpu/ggml-cpu-impl.h` - Include guards

---

## Current Status

### ‚úÖ Completed This Session
1. ‚úÖ Identified root cause: F32 tensors in AMX buffers
2. ‚úÖ Studied upstream buffer list architecture
3. ‚úÖ Implemented extra buffer types in ggml-backend.cpp
4. ‚úÖ Registered AMX as first extra buffer type
5. ‚úÖ Implemented buffer list infrastructure in llama.cpp
6. ‚úÖ Fixed `buft_input` and `buft_output` to not use AMX
7. ‚úÖ Created comprehensive documentation

### ‚úÖ Completed Previous Sessions
1. ‚úÖ Multi-threading barrier (atomic-based)
2. ‚úÖ Backend association (`is_host = true`)
3. ‚úÖ Tensor type filtering (only Q4_0/Q4_1 get AMX traits)
4. ‚úÖ Struct layout fix (`shared*` not `threadpool*`)
5. ‚úÖ Missing headers added

### ‚è≥ In Progress
- Buffer list integration into tensor loading

### ‚è∏Ô∏è Blocked Until Tensor Loading Complete
- Testing with AMX
- Performance benchmarks
- GPU hybrid mode testing

---

## Next Steps (Priority Order)

### Immediate (Next Session)

**1. Update Tensor Loading to Use Buffer Lists** (HIGHEST PRIORITY)

**Approach**: Keep it simple - don't change model structure
```cpp
// At layer setup time (line 4938):
for (int i = 0; i < i_gpu_start; ++i) {
    // Create buffer list
    buft_list_t buft_list = make_cpu_buft_list(use_extra_bufts, no_host);

    // Use first buffer type (AMX if available, else CPU)
    // This gives AMX for Q4_0/Q4_1 layers, CPU for F32 layers
    model.buft_layer[i] = buft_list[0];  // Simplified approach
}
```

**Better Approach**: Per-tensor selection during allocation
```cpp
// In llm_load_tensors(), when allocating each tensor:
ggml_backend_buffer_type_t buft = select_weight_buft(
    tensor,
    determine_op_for_tensor(tensor),  // MUL_MAT, ADD, etc.
    model.buft_layer[layer_idx]       // Now this would be buft_list_t
);
```

**Implementation Steps**:
1. Find tensor allocation code in `llm_load_tensors()`
2. Add per-tensor buffer type selection
3. Test that Q4_0/Q4_1 tensors get AMX, F32 tensors get CPU
4. Verify buffer count matches upstream (should see 3 buffers)

**2. Set `use_extra_bufts` and `no_host` Based on `--amx` Flag**

Current:
```cpp
bool use_amx_buffer = use_amx && !need_host_buffer;
```

Should be:
```cpp
bool use_extra_bufts = use_amx;  // Enable extra buffer types (AMX)
bool no_host = use_amx && (n_gpu_layers == 0);  // Skip host buffer if CPU-only
```

**3. Test and Verify**

```bash
# Rebuild
cd /home/ron/src/ik_llama.cpp/build
cmake --build . --target llama-cli -j 32

# Test CPU-only with AMX
numactl --cpunodebind=2 --membind=2 ./build/bin/llama-cli \
  -m /mnt/ssd2/AI/Qwen3_30B/Q4_0/Qwen3-30B-A3B-Thinking-2507-Q4_0.gguf \
  -t 32 -n 128 -p "Test" --amx

# Should see:
# - 3 buffers allocated (AMX, CPU_REPACK?, CPU_Mapped)
# - No NaN errors
# - Valid token generation
```

### Future (After Tensor Loading Works)

**4. Performance Testing**
- Benchmark prompt processing (should show AMX usage for M>1)
- Benchmark token generation (will use VNNI, not AMX tiles)
- Compare vs non-AMX baseline

**5. GPU Hybrid Mode**
- Test with `-ngl 20` (some layers on GPU)
- Verify CPU layers use AMX correctly
- Verify GPU<->CPU transfers work

**6. NUMA Optimization** (Optional)
- Port upstream's NUMA mirror buffer support
- Replicate AMX buffers across NUMA nodes
- Test multi-socket performance

---

## Key Insights from This Session

### 1. **The NaN Was Never in AMX Kernels**
- AMX kernels are 100% identical to upstream (proven working)
- The bug was in buffer allocation architecture
- F32 tensors forced into AMX buffers ‚Üí unsupported operations ‚Üí NaN

### 2. **Buffer Lists Are Essential**
- Single buffer type per layer doesn't work
- Need fallback mechanism: try AMX, fall back to CPU
- Each tensor should select appropriate buffer based on operation

### 3. **`--amx` Flag Should Work Like `--no-host`**
- `use_extra_bufts = true` ‚Üí Enable AMX
- `no_host = true` ‚Üí Skip host buffer, prefer AMX
- This mirrors upstream behavior

### 4. **Three Buffers Are Normal**
- AMX: Q4_0/Q4_1 weights (VNNI-packed)
- CPU_REPACK: Other quantized weights
- CPU_Mapped: F32 weights (mmap'd)
- Our 1GB allocation was WRONG - should be ~32GB total

### 5. **Architecture Difference Doesn't Matter**
- Our fork lacks `.device` field in `ggml_backend_buffer_type`
- But using `is_host = true` works fine as workaround
- The real issue was buffer selection logic, not struct layout

---

## Testing Checklist (For Next Session)

After implementing tensor loading update:

- [ ] Build succeeds
- [ ] AMX buffer type activates with `--amx`
- [ ] See 3 buffers allocated (not just 2)
- [ ] Total buffer size ~32GB (not 1GB)
- [ ] No NaN errors during model load
- [ ] First token generates correctly
- [ ] Multiple tokens generate correctly
- [ ] Verify outputs match non-AMX mode
- [ ] Performance test: prompt processing
- [ ] Performance test: token generation

---

## Documentation Files

1. **AMX_SESSION_CURRENT.md** (this file)
   - Current session status
   - What's working, what's not
   - Immediate next steps

2. **AMX_BUFFER_SYSTEM_PORT.md**
   - Detailed architecture comparison
   - Implementation log
   - Code snippets and explanations

3. **AMX_IMPLEMENTATIONS_COMPREHENSIVE_COMPARISON.md**
   - Comparison of 4 AMX implementations
   - Created by agent analysis
   - Reference for understanding AMX

4. **Previous Documentation** (from earlier sessions)
   - Build instructions
   - Performance expectations
   - Architecture differences

---

## Build Commands

```bash
# Clean rebuild
cd /home/ron/src/ik_llama.cpp/build
cmake --build . --target llama-cli -j 32

# Test CPU-only with AMX
numactl --cpunodebind=2 --membind=2 ./build/bin/llama-cli \
  -m /mnt/ssd2/AI/Qwen3_30B/Q4_0/Qwen3-30B-A3B-Thinking-2507-Q4_0.gguf \
  -t 32 -n 128 -p "Test prompt" --amx

# Test without AMX (baseline)
numactl --cpunodebind=2 --membind=2 ./build/bin/llama-cli \
  -m /mnt/ssd2/AI/Qwen3_30B/Q4_0/Qwen3-30B-A3B-Thinking-2507-Q4_0.gguf \
  -t 32 -n 128 -p "Test prompt"

# Monitor AMX tile usage (only for M>1 batches)
sudo perf stat -a -e exe.amx_busy sleep 30
```

---

## Critical Files Reference

### Our Implementation
- `ggml/src/ggml-backend.cpp` - Extra buffer types (733-772)
- `ggml/include/ggml-backend.h` - Declarations (128, 131)
- `src/llama.cpp` - Buffer list infrastructure (1785-1836)
- `src/llama.cpp` - Layer buffer setup (4938)
- `src/llama.cpp` - Tensor loading (needs update)

### Upstream Reference (numa_read_mirror branch)
- `/home/ron/src/llama.cpp/src/llama-model.cpp` - Buffer list system
- `/home/ron/src/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp` - Extra buffers registration

---

**Last Updated**: 2025-10-03 23:55
**Next Session**: Implement tensor loading buffer list integration
**Blocking Issue**: Tensor loading still uses single buffer type per layer
**Estimated Effort**: 2-4 hours for tensor loading update + testing
